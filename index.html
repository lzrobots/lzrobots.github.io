<!doctype html> 
<html lang="en"> 
<head> 
<meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
<title> Li Zhang </title> 
<meta name="HandheldFriendly" content="True"> 
<meta name="MobileOptimized" content="320"> 
<link rel="stylesheet" href="css/bootstrap.min.css"> 
<link rel="stylesheet" href="css/style.css"> 
<link rel="shortcut icon" type="image/png" href="https://www.campaign.ox.ac.uk/image/oxford-logo.png"> 
</head> 

<body> 
<div id="home" class="container-fluid"> 

<div class="row-center"> 
<h1>Li Zhang</h1> 
<p>Tenure-track Professor
</br>School of Data Science, Fudan University</p> 
<a href="mailto:lizhangfd@fudan.edu.cn">email</a> /
<a href="https://scholar.google.com/citations?user=-wOTCE8AAAAJ&hl=en" target="_blank">google scholar</a>
</div> 

<hr> 
<div class="row"> 
<p> I am a tenure-track Professor at the School of Data Science, Fudan University where I direct the <a href="https://fudan-zvg.github.io" target="_blank">Zhang Vision Group</a>. The aim of my group is to engage in state of the art research in computer vision and deep learning.
	Previously, I was a Research Scientist at Samsung AI Center Cambridge, and a Postdoctoral Research Fellow at the University of Oxford where I was supervised by professor <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=en" target="_blank">Philip H.S. Torr</a> and professor <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&hl=en" target="_blank">Andrew Zisserman</a>.
Prior to joining Oxford, I read my PhD in computer science under the supervision of professor <a href="https://scholar.google.com/citations?user=MeS5d4gAAAAJ&hl=en" target="_blank">Tao Xiang</a> at Queen Mary University of London.
</p> 
</div> 


<br> 
<div class="row"> 
<p>
	If you are highly creative, have top grades/coding skill and interested in joining <a href="https://fudan-zvg.github.io" target="_blank">my group</a> please do not hesitate to send me your CV and transcripts of grades.
</p> 

</div> 


<hr> <div class="row"> 
<h2> News </h2> 
<br> 


<li> Two papers to appear in ICLR 2023.</li> 
<li> <span class="distinction">Call for papers!</span> We are orgnising a <a href="https://e2ead.github.io/2023.html" target="_blank">CVPR 2023 workshop</a> on End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation.</li> 
<li> One paper to appear in AAAI 2023.</li> 
<li> I will be serving as an Area Chair for <a href="https://cvpr2023.thecvf.com" target="_blank">CVPR 2023</a>.</li> 
<li> I will be talking at <a href="https://ccai.caai.cn/#guest" target="_blank">CCAI</a> 2022 and <a href="http://www.prcv.cn" target="_blank">PRCV</a> 2022.</li> 
<li> One paper to appear in NeurIPS 2022.</li> 
<li> Our work DGMN is accepted by IEEE TPAMI.</li>
<li> Four papers to appear in ECCV 2022.</li> 
<li> Our DeepInteraction is ranked 1st at the <span class="distinction">nuScenes 3D detection</span> <a href="https://nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Any" target="_blank">leaderboard</a>.</li>
<li> Our work SiamMask is accepted by IEEE TPAMI.</li>
<li> One paper to appear in CVPR 2022.</li> 
<li> Our work <a href="https://fudan-zvg.github.io/SETR/" target="_blank">SETR</a> is ranked second at the <a href="https://www.paperdigest.org/2022/02/most-influential-cvpr-papers-2022-02/" target="_blank">most influential CVPR papers</a>.</li>
<li> Two papers to appear in NeurIPS 2021 (1 Spotlight and 1 Poster).</li> 
<!-- <li> Three papers to appear in ICCV 2021.</li> 
<li> Two papers are accepted by IEEE TIP.</li>
<li> One paper is accepted by IEEE TPAMI.</li>
<li> Elected to the <span class="distinction">Shanghai Science & Technology 35 Under 35</span>.</li>
<li> Four papers to appear in CVPR 2021.</li>  -->
<!-- <li> Rank 1st on the test set of <a href="http://sceneparsing.csail.mit.edu/eval/leaderboard.php" target="_blank">MIT Scene Parsing Benchmark ADE20K</a>.</li> 
<li> One paper to appear in AAAI 2021.</li> 
<li> Two papers (1 Oral and 1 Poster) to appear in ACCV 2020.</li> 
<li> One paper to appear in ACM MM 2020.</li> 
<li> Three papers (1 Spotlight and 2 Posters) to appear in ECCV 2020.</li> 
<li> Our team (SAIC, Cambridge) win 3rd (Seen) and 6th (Unseen) in <a href="https://epic-kitchens.github.io/2020-55.html#results" target="_blank">EPIC-Kitchens challenges 2020</a>.</li>  
<li> Four papers (1 Oral and 3 Posters) to appear in CVPR 2020.</li>  -->
</div> 

<hr> <div class="row"> 
<h2> Publications </h2> 
<br> 
<ul> 


<li> <span class="papertitle">S-NeRF: Neural Radiance Fields for Street Views</span>
<br> Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, Li Zhang,
<br> ICLR 2023 
[<a href="https://openreview.net/forum?id=gx2yJS-ENqI" target="_blank">paper</a>]
[<a href="https://ziyang-xie.github.io/s-nerf/" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation</span>
<br> Qiang Wan, Jiachen Lu, Zilong Huang, Gang Yu, Li Zhang,
<br> ICLR 2023 
[<a href="https://arxiv.org/abs/2301.13156" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SeaFormer" target="_blank">code</a>]	
</li> 


<li> <span class="papertitle">PolarFormer: Multi-camera 3D Object Detection with Polar Transformers</span>
<br> Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang,
<br> AAAI 2023 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2206.15398" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/PolarFormer" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">ImpDet: Exploring Implicit Fields for 3D Object Detection</span>
<br> Xuelin Qian, Li Wang, Yi Zhu, Li Zhang, Yanwei Fu, Xiangyang Xue,
<br> WACV 2023
[<a href="https://arxiv.org/abs/2203.17240" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Rethinking Local and Global Feature Representation for Dense Prediction</span>
<br> Mohan Chen, Li Zhang, Rui Feng, Xiangyang Xue, Jianfeng Feng, 
<br> Pattern Recognition 2023
</li> 


<li> <span class="papertitle">DeepInteraction: 3D Object Detection via Modality Interaction</span>
<br> Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, Li Zhang,
<br> NeurIPS 2022 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/DeepInteraction" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Dynamic Graph Message Passing Network for Visual Recognition</span>
<br> Li Zhang, Mohan Chen, Anurag Arnab, Xiangyang Xue, Philip H.S. Torr 
<br> IEEE TPAMI 2022
[<a href="https://arxiv.org/abs/2209.09760" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/DGMN2" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">When, Where and How does it fail? A Spatial-temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving</span>
<br> Junhong Wang, Yun Li, Zhaoyu Zhou, Chengshun Wang, Yijie Hou, Li Zhang, Xiangyang Xue, Michael Kamp, Xiaolong (Luke) Zhang, Siming Chen 
<br> IEEE TVCG 2022
</li> 


<li> <span class="papertitle">Learning Ego 3D Representation as Ray Tracing</span>
<br> Jiachen Lu, Zheyuan Zhou, Xiatian Zhu, Hang Xu, Li Zhang,
<br> ECCV 2022
[<a href="https://github.com/fudan-zvg/Ego3RT" target="_blank">code</a>]
[<a href="https://youtu.be/lMwcGUahlJg" target="_blank">demo</a>]
</li> 


<li> <span class="papertitle">Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling</span>
<br> Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng,
<br> ECCV 2022
[<a href="https://github.com/fudan-zvg/PDS" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">FashionViL: Fashion-Focused Vision-and-Language Representation Learning</span>
<br> Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang,
<br> ECCV 2022
[<a href="https://github.com/BrandonHanx/mmf/tree/dev_brandon" target="_blank">code</a>]
</li> 



<li> <span class="papertitle">RCLane: Relay Chain Prediction for Lane Detection</span>
<br> Shenghua Xu, Xinyue Cai, Bin Zhao, Li Zhang, Hang Xu, Yanwei Fu, Xiangyang Xue,
<br> ECCV 2022
</li> 




<li> <span class="papertitle">SiamMask: A Framework for Fast Online Object Tracking and Segmentation</span>
<br> Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H.S. Torr
<br> TPAMI 2022
[<a href="https://github.com/foolwood/SiamMask" target="_blank">code</a>]
</li> 




<li> <span class="papertitle">ONCE-3DLanes: Building Monocular 3D Lane Detection</span>
<br> Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu, Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, Li Zhang
<br> CVPR 2022
[<a href="https://once-3dlanes.github.io" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">SGM3D: Stereo Guided Monocular 3D Object Detection</span>
<br> Zheyuan Zhou, Liang Du, Xiaoqing Ye, Zhikang Zou, Xiao Tan, Errui Ding, Li Zhang, Xiangyang Xue, Jianfeng Feng,
<br> IEEE Robotics and Automation Letters (RAL) 2022,
[<a href="https://arxiv.org/abs/2112.01914" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">SOFT: Softmax-free Transformer with Linear Complexity</span>
<br> Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang,
<br> NeurIPS 2021 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/SOFT" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Progressive Coordinate Transforms for Monocular 3D Object Detection</span>
<br> Li Wang, Li Zhang, Yi Zhu, Zhi Zhang, Tong He, Mu Li, Xiangyang Xue,
<br> NeurIPS 2021
[<a href="https://github.com/amazon-research/progressive-coordinate-transforms" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection</span>
<br> Zhikang Zou, Xiaoqing Ye, Liang Du, Xianhui Cheng, Xiao Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, Errui Ding,
<br> ICCV 2021
</li> 

<li> <span class="papertitle">Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer</span>
<br> Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, Tao Xiang,
<br> ICCV 2021
[<a href="https://github.com/zhiheLu/CWT-for-FSS" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Boundary-sensitive Pretraining for Temporal Localization in Videos</span>
<br> Mengmeng Xu, Victor Escorcia, Brais Martínez, Juan-Manuel Perez-Rua, Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang,
<br> ICCV 2021
</li> 

<li> <span class="papertitle">Text-Based Person Search with Limited Data</span>
<br> Xiao Han, Sen He, Li Zhang, Tao Xiang,
<br> BMVC 2021
</li> 

<li> <span class="papertitle">Few-shot Action Recognition with Prototype-centered Attentive Learning</span>
<br> Xiatian Zhu, Antoine Toisoul, Juan-Manuel Prez-Ra, Li Zhang, Brais Martinez, Tao Xiang,
<br> BMVC 2021
</li> 


<li> <span class="papertitle">Rethinking Local and Global Feature Representation for Semantic Segmentation</span>
<br> Mohan Chen, Xinxuan Zhao, Bingfei Fu, Li Zhang, Xiangyang Xue,
<br> BMVC 2021
</li> 

<li> <span class="papertitle">Dual Prior Learning for Blind and Blended Image Restoration</span>
<br> Xin Jin, Li Zhang, Chaowei Shan, Xin Li, Zhibo Chen,
<br> IEEE TIP 2021
</li> 


<li> <span class="papertitle">Towards Efficient Scene Understanding via Squeeze Reasoning</span>
<br> Xiangtai Li, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin,
<br> IEEE TIP 2021
[<a href="https://github.com/lxtGH/SFSegNets1" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Global Aggregation then Local Distribution for Scene Parsing</span>
<br> Xiangtai Li, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang,
<br> IEEE TIP 2021
[<a href="https://github.com/lxtGH/GALD-DGCNet" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">How to trust unlabeled data? Instance Credibility Inference for Few-Shot Learning</span>
<br> Yikai Wang, Li Zhang, Yuan Yao, Yanwei Fu
<br> TPAMI 2021
[<a href="https://arxiv.org/pdf/2007.08461" target="_blank">paper</a>]
[<a href="https://github.com/Yikai-Wang/ICI-FSL" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers</span>
<br> Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, Li Zhang
<br> CVPR 2021
[<a href="https://arxiv.org/abs/2012.15840" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SETR" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection</span>
<br> Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, Li Zhang
<br> CVPR 2021
[<a href="https://arxiv.org/abs/2103.16470" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/DDMP" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning Dynamic Alignment via Meta-filter for Few-shot Learning</span>
<br> Chengming Xu, Yanwei Fu, Chen Liu, Chengjie Wang, Jilin Li, Feiyue Huang, Li Zhang, Xiangyang Xue, 
<br> CVPR 2021
[<a href="https://github.com/loadder/Dynamic-Meta-filter" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Delving into Data: Effectively Substitute Training for Black-box Attack</span>
<br> Wenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yanwei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, Xiangyang Xue
<br> CVPR 2021
</li> 


<li> <span class="papertitle">Learning a Few-shot Embedding Model with Contrastive Learning</span>
<br> Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, Li Zhang
<br> AAAI 2021
[<a href="https://github.com/corwinliu9669/Learning-a-Few-shot-Embedding-Model-with-Contrastive-Learning" target="_blank">code</a>]
</li> 



<li> <span class="papertitle">Long-Term Cloth-Changing Person Re-identification</span>
<br> Xuelin Qian, Wenxuan Wang, Li Zhang, Fangrui Zhu, Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue
<br> ACCV 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2005.12633" target="_blank">paper</a>]
[<a href="https://naiq.github.io/LTCC_Perosn_ReID.html" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">Dynamic Depth Fusion and Transformation for Monocular 3D Object Detection</span>
<br> Erli Ouyang*, Li Zhang*, Mohan Chen, Anurag Arnab, Yanwei Fu
<br> ACCV 2020
[<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Ouyang_Dynamic_Depth_Fusion_and_Transformation_for_Monocular_3D_Object_Detection_ACCV_2020_paper.pdf" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition</span>
<br> Yuqian Fu*, Li Zhang*, Junke Wang, Yanwei Fu, Yu-Gang Jiang
<br> ACM MM 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2010.09982" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">Few-shot Action Recognition with Permutation-invariant Attention</span>
<br> Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H.S. Torr, Piotr Koniusz
<br> ECCV 2020 <span class="distinction">(Spotlight)</span> 
[<a href="https://arxiv.org/abs/2001.03905" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">XingGAN for Person Image Generation</span>
<br> Hao Tang, Song Bai, Li Zhang, Philip H.S. Torr, Nicu Sebe
<br> ECCV 2020
[<a href="https://arxiv.org/abs/2007.09278" target="_blank">paper</a>]
[<a href="https://github.com/Ha0Tang/XingGAN" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Improving Semantic Segmentation via Decoupled Body and Edge Supervision</span>
<br> Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
<br> ECCV 2020
[<a href="https://arxiv.org/abs/2007.10035" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/DecoupleSegNets" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Dynamic Graph Message Passing Network</span>
<br> Li Zhang, Dan Xu, Anurag Arnab, Philip H.S. Torr
<br> CVPR 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/1908.06955" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/dgmn" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Strip Pooling: Rethinking Spatial Pooling for Scene Parsing</span>
<br> Qibin Hou, Li Zhang, Ming-Ming Cheng, Jiashi Feng
<br> CVPR 2020
[<a href="https://arxiv.org/abs/2003.13328" target="_blank">paper</a>]
[<a href="https://github.com/Andrew-Qibin/SPNet" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Instance Credibility Inference for Few-Shot Learning</span>
<br> Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu
<br> CVPR 2020 
[<a href="https://arxiv.org/abs/2003.11853" target="_blank">paper</a>]
[<a href="https://github.com/Yikai-Wang/ICI-FSL" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Style Normalization and Restitution for Generalizable Person Re-identification</span>
<br> Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Li Zhang
<br> CVPR 2020 
[<a href="https://arxiv.org/abs/2005.11037" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Dual Graph Convolutional Network for Semantic Segmentation</span>
<br> Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H.S. Torr
<br> BMVC 2019 
[<a href="https://arxiv.org/abs/1909.06121" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/GALD-Net" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Global Aggregation then Local Distribution in Fully Convolutional Networks</span>
<br> Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
<br> BMVC 2019 
[<a href="https://arxiv.org/abs/1909.07229" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/GALD-Net" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Fast Online Object Tracking and Segmentation: A Unifying Approach</span>
<br> Qiang Wang*, Li Zhang*, Luca Bertinetto*, Weiming Hu, Philip H.S. Torr
<br> CVPR 2019 
[<a href="https://arxiv.org/abs/1812.05050" target="_blank">paper</a>]
[<a href="https://github.com/foolwood/SiamMask" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning to Compare: Relation Network for Few-Shot Learning</span>
<br> Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales
<br> CVPR 2018
[<a href="https://arxiv.org/abs/1711.06025" target="_blank">paper</a>]
[<a href="https://github.com/songrotek/LearningToCompare_FSL" target="_blank">FSL code</a>]
[<a href="https://github.com/lzrobots/LearningToCompare_ZSL" target="_blank">ZSL code</a>]
</li> 


<li> <span class="papertitle">Learning a Deep Embedding Model for Zero-Shot Learning</span>
<br> Li Zhang, Tao Xiang, Shaogang Gong
<br> CVPR 2017
[<a href="https://arxiv.org/abs/1611.05088" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/DeepEmbeddingModel_ZSL" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning a Discriminative Null Space for Person Re-identification</span>
<br> Li Zhang, Tao Xiang, Shaogang Gong
<br> CVPR 2016
[<a href="http://arxiv.org/abs/1603.02139" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/NullSpace_ReID" target="_blank">code</a>]
</li> 

</ul> </div> 


<hr><div id="footer" class="row-footer"> 
<p>
&copy; <a href="https://lzrobots.github.io">Li Zhang</a> 2018-2023.
Based on a design by <a href="http://www.robots.ox.ac.uk/~namhoon/">Namhoon Lee</a>.
</p>
</div> 


</div>
</body> 
</html>






