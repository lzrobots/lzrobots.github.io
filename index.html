<!doctype html> 
<html lang="en"> 
<head> 
<meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> 
<title> Li Zhang </title> 
<meta name="HandheldFriendly" content="True"> 
<meta name="MobileOptimized" content="320"> 
<link rel="stylesheet" href="css/bootstrap.min.css"> 
<link rel="stylesheet" href="css/style.css"> 
<link rel="shortcut icon" type="image/png" href="https://www.campaign.ox.ac.uk/image/oxford-logo.png"> 
</head> 

<body> 
<div id="home" class="container-fluid"> 

<div class="row-center"> 
<h1>Li Zhang</h1> 
<p>Professor
</br>School of Data Science, Fudan University</p> 
<a href="mailto:lizhangfd@fudan.edu.cn">email</a> /
<a href="https://scholar.google.com/citations?user=-wOTCE8AAAAJ&hl=en" target="_blank">google scholar</a> /
<a href="https://github.com/fudan-zvg" target="_blank">github</a>
</div> 

<hr> 
<div class="row"> 
<p> I am a full professor in the School of Data Science at Fudan University. Previously, I was a Research Scientist at Samsung AI Center Cambridge, and a Postdoctoral Researcher at the Department of Engineering Science, University of Oxford, under the supervision of professor <a href="https://scholar.google.com/citations?user=kPxa2w0AAAAJ&hl=en" target="_blank">Philip H.S. Torr</a> and professor <a href="https://scholar.google.com/citations?user=UZ5wscMAAAAJ&hl=en" target="_blank">Andrew Zisserman</a>. Prior to joining Oxford, I read my PhD in computer science under the supervision of professor <a href="https://scholar.google.com/citations?user=MeS5d4gAAAAJ&hl=en" target="_blank">Tao Xiang</a> at Queen Mary University of London.
</p> 
<br> 
<p> My general research interests cover the broad area of deep learning and artificial intelligence, with special emphasis on building physical AI systems for <span class="papertitle">robotics</span> and <span class="papertitle">autonomous driving</span>.
</p> 
</div> 


<br> 
<div class="row"> 
<h4> To Prospective Students </h4> 
<p>
	If you are highly creative, have top grades/coding skill and interested in joining my group please do not hesitate to send me your CV and transcripts of grades.
	<br> 
	Make sure you are super <font color="#FF0000">enthusiastic</font> about AI research.
</p> 

</div> 


<hr> <div class="row"> 
<h2> News </h2> 
<br> 

<li> Two papers <a href="https://github.com/fudan-zvg/PVG" target="_blank">PVG</a> and <a href="https://github.com/fudan-zvg/PARTNER" target="_blank">PARTNER</a> are accepted by IJCV.
<li> I will be serving as an Area Chair for ECCV 2026</a>.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/Efficient4D" target="_blank">Efficient4D</a> is accepted by IJCV.
<li> One paper to appear in AAAI 2026.</li> 
<li> Our work <a href="https://brain-3d.github.io" target="_blank">Brain3D</a> is accepted by IJCV.
<li> Five papers to appear in NeurIPS 2025.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/RoadNet" target="_blank">RoadNetTransformer</a> is accepted by TPAMI.
<li> I will be serving as an Area Chair for CVPR 2026</a>.</li> 
<li> Promoted to Full Professor in December 2024.</li> 
<li> One paper to appear in ACM MM 2025.</li> 
<li> Two papers to appear in ICCV 2025.</li> 
<li> One paper to appear in ACL 2025.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/DeepInteraction" target="_blank">DeepInteraction++</a> is accepted by TPAMI.
<li> Elsevier Highly Cited Chinese Researchers.</li> 
<li> Our work <a href="https://arxiv.org/abs/2404.14671" target="_blank">LaneCorrect</a> is accepted by IJCV.
<li> Five papers to appear in CVPR 2025.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/PDS" target="_blank">PDS</a> is accepted by IJCV.
<li> I will be serving as an Area Chair for <a href="https://neurips.cc/" target="_blank">NeurIPS 2025</a>.</li> 
<li> Our work on autonomous driving simulation <a href="https://fudan-zvg.github.io/S-NeRF/" target="_blank">S-NeRF++</a> is accepted by IEEE TPAMI.</li>
<li> Four papers to appear in ICLR 2025.</li> 	
<li> Our work <a href="https://arxiv.org/abs/2301.13156" target="_blank">SeaFormer++</a> is accepted by IJCV.
<li> One paper to appear in ICASSP 2025.</li> 	
<li> Five papers (2 Spotlights and 3 Posters) to appear in NeurIPS 2024.</li> 	
<li> I will be serving as an Area Chair for <a href="https://cvpr.thecvf.com" target="_blank">CVPR 2025</a>.</li> 
<!-- <li> Our work <a href="https://arxiv.org/abs/2404.14671" target="_blank">LaneCorrect</a> is accepted by IJCV.</li> -->
<li> Three papers to appear in ECCV 2024.</li> 	
<li> Our work <a href="https://github.com/fudan-zvg/SETR" target="_bla nk">SETR</a> is accepted by IJCV.</li>
<li> One paper to appear in IROS 2024 as oral presentation. </li> 
<li> Two papers are accepted by IEEE RAL. </li>
<li> I will be serving as an Area Chair for <a href="https://nips.cc" target="_blank">NeurIPS 2024</a>.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/SOFT" target="_blank">SOFT</a> is accepted by IJCV.</li>
<li> Three papers to appear in ICLR 2024.</li> 	
<li> Appointed as Associate Editor (AE) for Pattern Recognition.</li> 	
<li> Two papers (1 Oral and 1 Poster) to appear in AAAI 2024.</li> 	
<!-- <li> Two papers (1 Oral and 1 Poster) to appear in ICCV 2023.</li> 
<li> I will be serving as an Area Chair for <a href="https://cvpr.thecvf.com/Conferences/2024" target="_blank">CVPR 2024</a>.</li> 
<li> One paper to appear in IROS 2023.</li> 
<li> One paper is accepted by Machine Vision and Applications.</li> 
<li> I will be serving as an Area Chair for <a href="https://nips.cc" target="_blank">NeurIPS 2023</a>.</li> 
<li> Two papers (1 Highlight and 1 Poster) to appear in CVPR 2023.</li> 
<li> Two papers to appear in ICLR 2023.</li> 
<li> <font color="#FF0000">Call for papers!</font> We are orgnising a <a href="https://e2ead.github.io/2023.html" target="_blank">CVPR 2023 workshop</a> on End-to-End Autonomous Driving: Perception, Prediction, Planning and Simulation.</li> 
<li> One paper (Oral) to appear in AAAI 2023.</li> 
<li> I will be serving as an Area Chair for <a href="https://cvpr2023.thecvf.com" target="_blank">CVPR 2023</a>.</li> 
<li> I will be talking at <a href="https://ccai.caai.cn/#guest" target="_blank">CCAI</a> 2022 and <a href="http://www.prcv.cn" target="_blank">PRCV</a> 2022.</li> 
<li> One paper (Spotlight) to appear in NeurIPS 2022.</li> 
<li> Our work <a href="https://github.com/fudan-zvg/DGMN2" target="_blank">DGMN</a> is accepted by IEEE TPAMI.</li>
<li> Four papers to appear in ECCV 2022.</li> 
<li> Our DeepInteraction is ranked 1st at the <span class="distinction">nuScenes 3D detection</span> <a href="https://nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Any" target="_blank">leaderboard</a>.</li>
<li> Our work SiamMask is accepted by IEEE TPAMI.</li>
<li> One paper to appear in CVPR 2022.</li> 
<li> Our work <a href="https://fudan-zvg.github.io/SETR/" target="_blank">SETR</a> is ranked second at the <a href="https://www.paperdigest.org/2022/02/most-influential-cvpr-papers-2022-02/" target="_blank">most influential CVPR papers</a>.</li>
<li> Two papers (1 Spotlight and 1 Poster) to appear in NeurIPS 2021.</li>  -->
<!-- <li> Three papers to appear in ICCV 2021.</li> 
<li> Two papers are accepted by IEEE TIP.</li>
<li> One paper is accepted by IEEE TPAMI.</li>
<li> Elected to the <span class="distinction">Shanghai Science & Technology 35 Under 35</span>.</li>
<li> Four papers to appear in CVPR 2021.</li>  -->
<!-- <li> Rank 1st on the test set of <a href="http://sceneparsing.csail.mit.edu/eval/leaderboard.php" target="_blank">MIT Scene Parsing Benchmark ADE20K</a>.</li> 
<li> One paper to appear in AAAI 2021.</li> 
<li> Two papers (1 Oral and 1 Poster) to appear in ACCV 2020.</li> 
<li> One paper to appear in ACM MM 2020.</li> 
<li> Three papers (1 Spotlight and 2 Posters) to appear in ECCV 2020.</li> 
<li> Our team (SAIC, Cambridge) win 3rd (Seen) and 6th (Unseen) in <a href="https://epic-kitchens.github.io/2020-55.html#results" target="_blank">EPIC-Kitchens challenges 2020</a>.</li>  
<li> Four papers (1 Oral and 3 Posters) to appear in CVPR 2020.</li>  --> 
</div> 

<hr> <div class="row"> 
<h2> Preprints </h2> 
<br> 
<ul> 


<li> <span class="papertitle">Reinforcing Action Policies by Prophesying,</span>
<br> Jiahui Zhang, Ze Huang, Chun Gu, Zipei Ma, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://logosroboticsgroup.github.io/ProphRL" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">UniUGG: Unified 3D Understanding and Generation via Geometric-Semantic Encoding,</span>
<br> Yueming Xu, Jiahui Zhang, Ze Huang, Yurui Chen, Yanpeng Zhou, Zhenyu Chen, Yu-Jie Yuan, Pengxiang Xia, Guowei Huang, Xinyue Cai, Zhongang Qi, Xingyue Quan, Jianye Hao, Hang Xu, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://fudan-zvg.github.io/UniUGG" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving,</span>
<br> Jingyu Li, Bozhou Zhang, Xin Jin, Jiankang Deng, Xiatian Zhu, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://github.com/fudan-zvg/ImagiDrive" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">TexVerse: A Universe of 3D Objects with High-Resolution Textures,</span>
<br> Yibo Zhang, Li Zhang, Rui Ma, Nan Cao,
<br> arXiv preprint, 2025
[<a href="https://github.com/yiboz2001/TexVerse" target="_blank">github</a>]
[<a href="https://huggingface.co/datasets/YiboZhang2001/TexVerse" target="_blank">huggingface</a>]
</li> 

<li> <span class="papertitle">LMAD: Integrated End-to-End Vision-Language Model for Explainable Autonomous Driving,</span>
<br> Nan Song, Bozhou Zhang, Xiatian Zhu, Jiankang Deng, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://github.com/fudan-zvg/LMAD" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Relative Position Matters: Trajectory Prediction and Planning with Polar Representation,</span>
<br> Bozhou Zhang, Nan Song, Bingzhao Gao, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://github.com/fudan-zvg/Polaris" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">KFFocus: Highlighting Keyframes for Enhanced Video Understanding,</span>
<br> Ming Nie, Chunwei Wang, Hang Xu, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://www.arxiv.org/abs/2508.08989" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">RealEngine: Simulating Autonomous Driving in Realistic Context,</span>
<br> Junzhe Jiang, Nan Song, Jingyu Li, Xiatian Zhu, Li Zhang,
<br> arXiv preprint, 2025
[<a href=" https://github.com/fudan-zvg/RealEngine" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">DeMo++: Motion Decoupling for Autonomous Driving,</span>
<br> Bozhou Zhang, Nan Song, Xiatian Zhu, Li Zhang,
<br> arXiv preprint, 2025
[<a href="https://github.com/fudan-zvg/DeMo" target="_blank">project page</a>]
</li> 
	

<li> <span class="papertitle">4D Gaussian Splatting: Modeling Dynamic Scenes with Native 4D Primitives,</span>
<br> Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang, Jianfeng Feng, Yu-Gang Jiang, Philip H.S. Torr,
<br> arXiv preprint, 2024
[<a href="https://arxiv.org/abs/2412.20720" target="_blank">paper</a>]
</li> 


<!-- <li> <span class="papertitle">Urban Scene Diffusion through Semantic Occupancy Map,</span>
<br> Junge Zhang, Qihang Zhang, Li Zhang, Ramana Rao Kompella, Gaowen Liu, Bolei Zhou,
<br> arXiv preprint, 2024
[<a href="https://metadriverse.github.io/urbandiff/" target="_blank">project page</a>]
[<a href="https://arxiv.org/abs/2403.11697" target="_blank">paper</a>]
</li>  -->



<!-- <li> <span class="papertitle">Harnessing Diffusion Models for Visual Perception with Meta Prompts,</span>
<br> Qiang Wan, Ming Nie, Zilong Huang, Bingyi Kang, Jiashi Feng, Li Zhang,
<br> arXiv preprint, 2023
[<a href="https://github.com/fudan-zvg/meta-prompts" target="_blank">code</a>]
</li> 
 -->
	

</ul> </div> 

<br> 

<hr> <div class="row"> 
<h2> Journals </h2> 
<br> 
<ul> 

<li> <span class="papertitle">Periodic Vibration Gaussian: Dynamic Urban Scene Reconstruction and Real-time
Rendering,</span>
<br> Yurui Chen, Chun Gu, Junzhe Jiang, Xiatian Zhu, Li Zhang,
<br> IJCV 2026
[<a href="https://fudan-zvg.github.io/PVG/" target="_blank">project page</a>]
[<a href="https://github.com/fudan-zvg/PVG" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">PARTNER: Level up the Polar Representation for 3D Object Detection,</span>
<br> Ming Nie, Chunwei Wang, Hang Xu, Li Zhang,
<br> IJCV 2026
[<a href="https://github.com/fudan-zvg/PARTNER" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Efficient4D: Fast Dynamic 3D Object Generation from a Single-view Video,</span>
<br> Zijie Pan, Zeyu Yang, Xiatian Zhu, Li Zhang,
<br> IJCV 2026
[<a href="https://fudan-zvg.github.io/Efficient4D/" target="_blank">project page</a>]
[<a href="https://github.com/fudan-zvg/Efficient4D" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Brain3D: Generating 3D Objects from fMRI,</span>
<br> Yuankun Yang, Li Zhang, Ziyang Xie, Zhiyuan Yuan, Jianfeng Feng, Xiatian Zhu, Yu-Gang Jiang,
<br> IJCV 2026
[<a href="https://arxiv.org/abs/2405.15239" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Translating Images to Road Network: A Sequence-to-Sequence Perspective,</span>
<br> Jiachen Lu, Ming Nie, Bozhou Zhang, Renyuan Peng, Xinyue Cai, Hang Xu, Feng Wen, Wei Zhang, Li Zhang,
<br> IEEE TPAMI 2025
[<a href="https://github.com/fudan-zvg/RoadNet" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">DeepInteraction++: Multi-Modality Interaction for Autonomous Driving,</span>
<br> Zeyu Yang, Nan Song, Wei Li, Xiatian Zhu, Li Zhang, Philip HS Torr,
<br> IEEE TPAMI 2025
[<a href="https://github.com/fudan-zvg/DeepInteraction" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and Generation,</span>
<br> Yurui Chen, Junge Zhang, Ziyang Xie, Wenye Li, Feihu Zhang, Jiachen Lu, Li Zhang,
<br> IEEE TPAMI 2025
[<a href="https://fudan-zvg.github.io/S-NeRF/" target="_blank">project page</a>]
[<a href="https://arxiv.org/abs/2402.02112" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/S-NeRF" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">LaneCorrect: Self-supervised Lane Detection,</span>
<br> Ming Nie, Xinyue Cai, Hang Xu, Li Zhang,
<br> IJCV 2025
[<a href="https://arxiv.org/abs/2404.14671" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">Preconditioned Score-based Generative Models,</span>
<br> Hengyuan Ma, Xiatian Zhu, Jianfeng Feng, Li Zhang
<br> IJCV 2025
[<a href="https://arxiv.org/abs/2302.06504" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/PDS" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">SeaFormer++: Squeeze-enhanced Axial Transformer for Mobile Visual Recognition,</span>
<br> Qiang Wan, Zilong Huang, Jiachen Lu, Gang Yu, Li Zhang,
<br> IJCV 2025
[<a href="https://arxiv.org/abs/2301.13156" target="_blank">paper</a>]
</li> 




<li> <span class="papertitle">Vision Transformers: From Semantic Segmentation to Dense Prediction,</span>
<br> Li Zhang, Jiachen Lu, Sixiao Zheng, Xinxuan Zhao, Xiatian Zhu, Yanwei Fu, Tao Xiang, Jianfeng Feng, Philip H.S. Torr,
<br> IJCV 2024
[<a href="https://arxiv.org/abs/2207.09339" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SETR" target="_blank">code</a>]
</li> 



<li> <span class="papertitle">RoDyn-SLAM: Robust Dynamic Dense RGB-D SLAM with Neural Radiance Fields,</span>
<br> Haochen Jiang, Yueming Xu, Kejie Li, Jianfeng Feng, Li Zhang,
<br> IEEE Robotics and Automation Letters (RA-L), 2024
[<a href="https://github.com/fudan-zvg/Rodyn-SLAM" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">A Hybrid Approach for Cross-modality Pose Estimation Between Image and Point Cloud,</span>
<br> Ze Huang, Li Sun, Qibin He, Zhongyang Xiao, Xinhui Bai, Hongyuan Yuan, Song-Zhi Su, Li Zhang, 
<br> IEEE Robotics and Automation Letters (RA-L), 2024
</li> 
	
<li> <span class="papertitle">Softmax-free Linear Transformers,</span>
<br> Jiachen Lu, Junge Zhang, Xiatian Zhu, Jianfeng Feng, Tao Xiang, Li Zhang, 
<br> IJCV 2024
[<a href="https://arxiv.org/abs/2207.03341" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SOFT" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Persistent Object Identification Leveraging Non-Visual Markers,</span>
<br> Michael PJ Camilleri, Li Zhang, Rasneer S Bains, Andrew Zisserman, Christopher KI Williams,
<br> Machine Vision and Applications 2023 
</li>

<li> <span class="papertitle">Dynamic Graph Message Passing Network for Visual Recognition,</span>
<br> Li Zhang, Mohan Chen, Anurag Arnab, Xiangyang Xue, Philip H.S. Torr 
<br> IEEE TPAMI 2023
[<a href="https://arxiv.org/abs/2209.09760" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/DGMN2" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">SiamMask: A Framework for Fast Online Object Tracking and Segmentation,</span>
<br> Weiming Hu, Qiang Wang, Li Zhang, Luca Bertinetto, Philip H.S. Torr
<br> IEEE TPAMI 2023
[<a href="https://github.com/foolwood/SiamMask" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Rethinking Local and Global Feature Representation for Dense Prediction,</span>
<br> Mohan Chen, Li Zhang, Rui Feng, Xiangyang Xue, Jianfeng Feng, 
<br> Pattern Recognition 2023
</li> 


<li> <span class="papertitle">When, Where and How does it fail? A Spatial-temporal Visual Analytics Approach for Interpretable Object Detection in Autonomous Driving,</span>
<br> Junhong Wang, Yun Li, Zhaoyu Zhou, Chengshun Wang, Yijie Hou, Li Zhang, Xiangyang Xue, Michael Kamp, Xiaolong (Luke) Zhang, Siming Chen 
<br> IEEE TVCG 2022
</li> 

<li> <span class="papertitle">SGM3D: Stereo Guided Monocular 3D Object Detection,</span>
<br> Zheyuan Zhou, Liang Du, Xiaoqing Ye, Zhikang Zou, Xiao Tan, Errui Ding, Li Zhang, Xiangyang Xue, Jianfeng Feng,
<br> IEEE Robotics and Automation Letters (RAL) 2022,
[<a href="https://arxiv.org/abs/2112.01914" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Dual Prior Learning for Blind and Blended Image Restoration,</span>
<br> Xin Jin, Li Zhang, Chaowei Shan, Xin Li, Zhibo Chen,
<br> IEEE TIP 2021
</li> 


<li> <span class="papertitle">Towards Efficient Scene Understanding via Squeeze Reasoning,</span>
<br> Xiangtai Li, Xia Li, Ansheng You, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Zhouchen Lin,
<br> IEEE TIP 2021
[<a href="https://github.com/lxtGH/SFSegNets1" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Global Aggregation then Local Distribution for Scene Parsing,</span>
<br> Xiangtai Li, Li Zhang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, Xiatian Zhu, Tao Xiang,
<br> IEEE TIP 2021
[<a href="https://github.com/lxtGH/GALD-DGCNet" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">How to trust unlabeled data? Instance Credibility Inference for Few-Shot Learning,</span>
<br> Yikai Wang, Li Zhang, Yuan Yao, Yanwei Fu
<br> IEEE TPAMI 2021
[<a href="https://arxiv.org/pdf/2007.08461" target="_blank">paper</a>]
[<a href="https://github.com/Yikai-Wang/ICI-FSL" target="_blank">code</a>]
</li> 

</ul> </div> 

<br> 

<hr> <div class="row"> 
<h2> Conferences </h2> 
<br> 
<ul> 

<li> <span class="papertitle">Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving,</span>
<br> Bozhou Zhang, Jingyu Li, Nan Song, Li Zhang,
<br> AAAI 2026
[<a href="https://github.com/fudan-zvg/VeteranAD" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration,</span>
<br> Jiahui Zhang, Yurui Chen, Yueming Xu, Ze Huang, Yanpeng Zhou, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang,
<br> NeurIPS 2025
[<a href="https://github.com/fudan-zvg/4D-VLA" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">Towards Unified Multimodal Interleaved Generation via Group Relative Policy Optimization,</span>
<br> Ming Nie, Chunwei Wang, Jianhua Han, Hang Xu, Li Zhang,
<br> NeurIPS 2025
</li> 


<li> <span class="papertitle">UniMotion: A Unified Motion Framework for Simulation, Prediction and Planning,</span>
<br> Nan Song, Junzhe Jiang, Jingyu Li, Xiatian Zhu, Li Zhang,
<br> NeurIPS 2025
</li> 

<li> <span class="papertitle">Future-Aware End-to-End Driving: Bidirectional Modeling of Trajectory Planning and Scene Evolution,</span>
<br> Bozhou Zhang, Nan Song, Jingyu Li, Xiatian Zhu, Jiankang Deng, Li Zhang,
<br> NeurIPS 2025
</li> 


<li> <span class="papertitle">From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D,</span>
<br> Jiahui Zhang, Yurui Chen, Yanpeng Zhou, Yueming Xu, Ze Huang, Jilin Mei, Junhui Chen, Yu-Jie Yuan, Xinyue Cai, Guowei Huang, Xingyue Quan, Hang Xu, Li Zhang,
<br> NeurIPS 2025 (Datasets and Benchmarks Track)
[<a href="https://fudan-zvg.github.io/spar" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting,</span>
<br> Zipei Ma, Junzhe Jiang, Yurui Chen, Li Zhang,
<br> ICCV 2025
[<a href="https://github.com/fudan-zvg/BezierGS" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">Driving Scene Synthesis on Free-form Trajectories with Generative Prior,</span>
<br> Zeyu Yang, Zijie Pan, Yuankun Yang, Xiatian Zhu, Li Zhang,
<br> ICCV 2025
[<a href="https://arxiv.org/abs/2412.01717" target="_blank">paper</a>]
[<a href="https://fudan-zvg.github.io/DriveX" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">MS-Road: Towards Spatiotemporal-Consistent Large-Scale Road Reconstruction,</span>
<br> Ze Huang, Zhongyang Xiao, Mingliang Song, Yu Fang, Hongyuan Yuan, Kevin Li Sun, Li Zhang,
<br> ACM MM 2025
</li> 

<li> <span class="papertitle">Data Interpreter: An LLM Agent for Data Science,</span>
<br> Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Ceyao Zhang, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, et al.
<br> ACL Findings 2025
</li>

<li> <span class="papertitle">Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning,</span>
<br> Bozhou Zhang, Nan Song, Xin Jin, Li Zhang,
<br> CVPR 2025
[<a href="https://github.com/fudan-zvg/BridgeAD" target="_blank">code</a>]
</li> 
	
<li> <span class="papertitle">IRGS: Inter-Reflective Gaussian Splatting with 2D Gaussian Ray Tracing,</span>
<br> Chun Gu, Xiaofei Wei, Zixuan Zeng, Yuxuan Yao, Li Zhang,
<br> CVPR 2025
[<a href="https://github.com/fudan-zvg/IRGS" target="_blank">code</a>]
[<a href="https://fudan-zvg.github.io/IRGS/" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">TensoFlow: Tensorial Flow-based Sampler for Inverse Rendering,</span>
<br> Chun Gu, Xiaofei Wei, Li Zhang, Xiatian Zhu,
<br> CVPR 2025
[<a href="https://github.com/fudan-zvg/tensoflow" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">UniScene: Unified Occupancy-centric Driving Scene Generation,</span>
<br> Bohan Li, Jiazhe Guo, Hongsi Liu, Yingshuang Zou, Yikang Ding, Xiwu Chen, Hu Zhu, Feiyang Tan, Chi Zhang, Tiancai Wang, Shuchang Zhou, Li Zhang, Xiaojuan Qi, Hao Zhao, Mu Yang, Wenjun Zeng, Xin Jin,
<br> CVPR 2025
[<a href="https://arlo0o.github.io/uniscene/" target="_blank">project page</a>]
</li> 

<li> <span class="papertitle">Improving Gaussian Splatting with Localized Points Management,</span>
<br> Haosen Yang, Chenhao Zhang, Wenqing Wang, Marco Volino, Adrian Hilton, Li Zhang, Xiatian Zhu,
<br> CVPR 2025 <span class="distinction">(Highlight)</span> 
</li> 

<li> <span class="papertitle">Diffusion<sup>2</sup>: Dynamic 3D Content Generation via Score Composition of Orthogonal Diffusion Models,</span>
<br> Zeyu Yang, Zijie Pan, Chun Gu, Li Zhang,
<br> ICLR 2025
[<a href="https://github.com/fudan-zvg/diffusion-square" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">GS-LiDAR: Generating Realistic LiDAR Point Clouds with Panoramic Gaussian Splatting,</span>
<br> Junzhe Jiang, Chun Gu, Yurui Chen, Li Zhang,
<br> ICLR 2025
[<a href="https://github.com/fudan-zvg/GS-LiDAR" target="_blank">project page</a>]
</li>

<li> <span class="papertitle">FreqPrior: Improving Video Diffusion Models with Frequency Filtering Gaussian Noise,</span>
<br> Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang,
<br> ICLR 2025
[<a href="https://github.com/fudan-zvg/FreqPrior" target="_blank">code</a>]
</li>

<li> <span class="papertitle">Reflective Gaussian Splatting,</span>
<br> Yuxuan Yao, Zixuan Zeng, Chun Gu, Xiatian Zhu, Li Zhang,
<br> ICLR 2025
[<a href="https://fudan-zvg.github.io/ref-gaussian/" target="_blank">project page</a>]
</li>

<li> <span class="papertitle">Brick-Diffusion: Generating Long Videos with Brick-to-Wall Denoising,</span>
<br> Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Hang Xu, Li Zhang,
<br> ICASSP 2025
[<a href="https://arxiv.org/abs/2501.02741" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">Tetrahedron Splatting for 3D Generation,</span>
<br> Chun Gu, Zeyu Yang, Zijie Pan, Xiatian Zhu, Li Zhang,
<br> NeurIPS 2024 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/tet-splatting" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Motion Forecasting in Continuous Driving,</span>
<br> Nan Song, Bozhou Zhang, Xiatian Zhu, Li Zhang,
<br> NeurIPS 2024 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/RealMotion" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">DeMo: Decoupling Motion Forecasting into Directional Intentions and Dynamic States,</span>
<br> Bozhou Zhang, Nan Song, Li Zhang,
<br> NeurIPS 2024
[<a href="https://github.com/fudan-zvg/DeMo" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">DG-SLAM: Robust Dynamic Gaussian Splatting SLAM with Hybrid Pose Optimization,</span>
<br> Yueming Xu, Haochen Jiang, Zhongyang Xiao, Jianfeng Feng, Li Zhang,
<br> NeurIPS 2024
[<a href="https://github.com/fudan-zvg/DG-SLAM" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM,</span>
<br> Ming Nie, Dan Ding, Chunwei Wang, Yuanfan Guo, Jianhua Han, Hang Xu, Li Zhang,
<br> NeurIPS 2024
[<a href="https://github.com/fudan-zvg/SlowFocus" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous
Driving,</span>
<br> Ming Nie, Renyuan Peng, Chunwei Wang, Xinyue Cai, Jianhua Han, Hang Xu, Li Zhang,
<br> ECCV 2024
[<a href="https://github.com/fudan-zvg/reason2drive" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation,</span>
<br> Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, Li Zhang,
<br> ECCV 2024
[<a href="https://github.com/fudan-zvg/WoVoGen" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Relightable 3D Gaussian: Real-time Point Cloud Relighting with BRDF Decomposition and Ray Tracing,</span>
<br> Jian Gao, Chun Gu, Youtian Lin, Hao Zhu, Xun Cao, Li Zhang, Yao Yao,
<br> ECCV 2024
[<a href="https://nju-3dv.github.io/projects/Relightable3DGaussian/" target="_blank">project page</a>]
[<a href="https://github.com/NJU-3DV/Relightable3DGaussian" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">OpenOcc: Open Vocabulary 3D Scene Reconstruction via Occupancy Representation,</span>
<br> Haochen Jiang, Yueming Xu, Yihan Zeng, Hang Xu, Wei Zhang, Jianfeng Feng, Li Zhang,
<br> IROS 2024 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2403.11796" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">Real-time Photorealistic Dynamic Scene Representation and Rendering with 4D Gaussian Splatting,</span>
<br> Zeyu Yang, Hongye Yang, Zijie Pan, Li Zhang,
<br> ICLR 2024
[<a href="https://fudan-zvg.github.io/4d-gaussian-splatting/" target="_blank">project page</a>]
[<a href="https://github.com/fudan-zvg/4d-gaussian-splatting" target="_blank">code</a>]
</li> 
	
<li> <span class="papertitle">Enhancing High-Resolution 3D Generation through Pixel-wise Gradient Clipping,</span>
<br> Zijie Pan, Jiachen Lu, Xiatian Zhu, Li Zhang,
<br> ICLR 2024
[<a href="https://fudan-zvg.github.io/PGC-3D/" target="_blank">project page</a>]
[<a href="https://github.com/fudan-zvg/PGC-3D" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Consistent4D: Consistent 360 Dynamic Object Generation from Monocular Video,</span>
<br> Yanqin Jiang, Li Zhang, Jin Gao, Weiming Hu, Yao Yao,
<br> ICLR 2024
[<a href="https://consistent4d.github.io" target="_blank">project page</a>]
[<a href="https://github.com/yanqinJiang/Consistent4D" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">S-Agents: Self-organizing Agents in Open-ended Environments,</span>
<br> Jiaqi Chen, Yuxian Jiang, Jiachen Lu, Li Zhang,
<br> ICLR workshop on Large Language Model (LLM) Agents, 2024
[<a href="https://arxiv.org/abs/2402.04578" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/S-Agents" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields,</span>
<br> Junge Zhang, Feihu Zhang, Shaochen Kuang, Li Zhang,
<br> AAAI 2024 <span class="distinction">(Oral)</span> 
[<a href="https://github.com/fudan-zvg/NeRF-LiDAR" target="_blank">code</a>]
</li> 	

	
<li> <span class="papertitle">LaneGraph2Seq: Lane Topology Extraction with Language Model via Vertex-Edge Encoding and Connectivity Enhancement,</span>
<br> Renyuan Peng, Xinyue Cai, Hang Xu, Jiachen Lu, Feng Wen, Wei Zhang, Li Zhang,
<br> AAAI 2024
[<a href="https://github.com/fudan-zvg/RoadNet" target="_blank">code</a>]	
</li> 	


	
<li> <span class="papertitle">Translating Images to Road Network: A Non-Autoregressive Sequence-to-Sequence Approach,</span>
<br> Jiachen Lu, Renyuan Peng, Xinyue Cai, Hang Xu, Hongyang Li, Feng Wen, Wei Zhang, Li Zhang,
<br> ICCV 2023 <span class="distinction">(Oral)</span> 
[<a href="https://github.com/fudan-zvg/RoadNet" target="_blank">code</a>]
</li> 	

<li> <span class="papertitle">PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection,</span>
<br> Ming Nie, Yujing Xue, Chunwei Wang, Chaoqiang Ye, Hang Xu, Xinge Zhu, Qingqiu Huang, Michael Bi Mi, Xinchao Wang, Li Zhang,
<br> ICCV 2023 
[<a href="https://github.com/fudan-zvg/PARTNER" target="_blank">code</a>]
</li> 	

<li> <span class="papertitle">SUIT: Learning Significance-guided Information for 3D Temporal Detection,</span>
<br> Zheyuan Zhou, Jiachen Lu, Yihan Zeng, Hang Xu, Li Zhang,
<br> IROS 2023 <span class="distinction">(Oral)</span> 
</li> 	

	
<li> <span class="papertitle">Generative Semantic Segmentation,</span>
<br> Jiaqi Chen, Jiachen Lu, Xiatian Zhu, Li Zhang,
<br> CVPR 2023 
[<a href="https://github.com/fudan-zvg/GSS" target="_blank">code</a>]
</li> 		
	
<li> <span class="papertitle">FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks,</span>
<br> Xiao Han, Xiatian Zhu, Licheng Yu, Li Zhang, Yi-Zhe Song, Tao Xiang,
<br> CVPR 2023 <span class="distinction">(Highlight)</span> 
[<a href="https://github.com/BrandonHanx/FAME-ViL" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">S-NeRF: Neural Radiance Fields for Street Views,</span>
<br> Ziyang Xie, Junge Zhang, Wenye Li, Feihu Zhang, Li Zhang,
<br> ICLR 2023 
[<a href="https://ziyang-xie.github.io/s-nerf/" target="_blank">project page</a>]
[<a href="https://github.com/fudan-zvg/S-NeRF" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">SeaFormer: Squeeze-enhanced Axial Transformer for Mobile Semantic Segmentation,</span>
<br> Qiang Wan, Jiachen Lu, Zilong Huang, Gang Yu, Li Zhang,
<br> ICLR 2023 
[<a href="https://arxiv.org/abs/2301.13156" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SeaFormer" target="_blank">code</a>]	
</li> 


<li> <span class="papertitle">PolarFormer: Multi-camera 3D Object Detection with Polar Transformers,</span>
<br> Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang,
<br> AAAI 2023 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2206.15398" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/PolarFormer" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">ImpDet: Exploring Implicit Fields for 3D Object Detection,</span>
<br> Xuelin Qian, Li Wang, Yi Zhu, Li Zhang, Yanwei Fu, Xiangyang Xue,
<br> WACV 2023
[<a href="https://arxiv.org/abs/2203.17240" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">DeepInteraction: 3D Object Detection via Modality Interaction,</span>
<br> Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, Li Zhang,
<br> NeurIPS 2022 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/DeepInteraction" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning Ego 3D Representation as Ray Tracing,</span>
<br> Jiachen Lu, Zheyuan Zhou, Xiatian Zhu, Hang Xu, Li Zhang,
<br> ECCV 2022
[<a href="https://github.com/fudan-zvg/Ego3RT" target="_blank">code</a>]
[<a href="https://youtu.be/lMwcGUahlJg" target="_blank">demo</a>]
</li> 


<li> <span class="papertitle">Accelerating Score-based Generative Models with Preconditioned Diffusion Sampling,</span>
<br> Hengyuan Ma, Li Zhang, Xiatian Zhu, Jianfeng Feng,
<br> ECCV 2022
[<a href="https://github.com/fudan-zvg/PDS" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">FashionViL: Fashion-Focused Vision-and-Language Representation Learning,</span>
<br> Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang,
<br> ECCV 2022
[<a href="https://github.com/BrandonHanx/mmf/tree/dev_brandon" target="_blank">code</a>]
</li> 



<li> <span class="papertitle">RCLane: Relay Chain Prediction for Lane Detection,</span>
<br> Shenghua Xu, Xinyue Cai, Bin Zhao, Li Zhang, Hang Xu, Yanwei Fu, Xiangyang Xue,
<br> ECCV 2022
</li> 



<li> <span class="papertitle">ONCE-3DLanes: Building Monocular 3D Lane Detection,</span>
<br> Fan Yan, Ming Nie, Xinyue Cai, Jianhua Han, Hang Xu, Zhen Yang, Chaoqiang Ye, Yanwei Fu, Michael Bi Mi, Li Zhang
<br> CVPR 2022
[<a href="https://once-3dlanes.github.io" target="_blank">project page</a>]
</li> 



<li> <span class="papertitle">SOFT: Softmax-free Transformer with Linear Complexity,</span>
<br> Jiachen Lu, Jinghan Yao, Junge Zhang, Xiatian Zhu, Hang Xu, Weiguo Gao, Chunjing Xu, Tao Xiang, Li Zhang,
<br> NeurIPS 2021 <span class="distinction">(Spotlight)</span> 
[<a href="https://github.com/fudan-zvg/SOFT" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Progressive Coordinate Transforms for Monocular 3D Object Detection,</span>
<br> Li Wang, Li Zhang, Yi Zhu, Zhi Zhang, Tong He, Mu Li, Xiangyang Xue,
<br> NeurIPS 2021
[<a href="https://github.com/amazon-research/progressive-coordinate-transforms" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">The Devil is in the Task: Exploiting Reciprocal Appearance-Localization Features for Monocular 3D Object Detection,</span>
<br> Zhikang Zou, Xiaoqing Ye, Liang Du, Xianhui Cheng, Xiao Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, Errui Ding,
<br> ICCV 2021
</li> 

<li> <span class="papertitle">Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer,</span>
<br> Zhihe Lu, Sen He, Xiatian Zhu, Li Zhang, Yi-Zhe Song, Tao Xiang,
<br> ICCV 2021
[<a href="https://github.com/zhiheLu/CWT-for-FSS" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Boundary-sensitive Pretraining for Temporal Localization in Videos,</span>
<br> Mengmeng Xu, Victor Escorcia, Brais Martínez, Juan-Manuel Perez-Rua, Xiatian Zhu, Li Zhang, Bernard Ghanem, Tao Xiang,
<br> ICCV 2021
</li> 

<li> <span class="papertitle">Text-Based Person Search with Limited Data,</span>
<br> Xiao Han, Sen He, Li Zhang, Tao Xiang,
<br> BMVC 2021
</li> 

<li> <span class="papertitle">Few-shot Action Recognition with Prototype-centered Attentive Learning,</span>
<br> Xiatian Zhu, Antoine Toisoul, Juan-Manuel Prez-Ra, Li Zhang, Brais Martinez, Tao Xiang,
<br> BMVC 2021
</li> 


<li> <span class="papertitle">Rethinking Local and Global Feature Representation for Semantic Segmentation,</span>
<br> Mohan Chen, Xinxuan Zhao, Bingfei Fu, Li Zhang, Xiangyang Xue,
<br> BMVC 2021
</li> 


<li> <span class="papertitle">Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers,</span>
<br> Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, Li Zhang
<br> CVPR 2021
[<a href="https://arxiv.org/abs/2012.15840" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/SETR" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Depth-conditioned Dynamic Message Propagation for Monocular 3D Object Detection,</span>
<br> Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, Li Zhang
<br> CVPR 2021
[<a href="https://arxiv.org/abs/2103.16470" target="_blank">paper</a>]
[<a href="https://github.com/fudan-zvg/DDMP" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning Dynamic Alignment via Meta-filter for Few-shot Learning,</span>
<br> Chengming Xu, Yanwei Fu, Chen Liu, Chengjie Wang, Jilin Li, Feiyue Huang, Li Zhang, Xiangyang Xue, 
<br> CVPR 2021
[<a href="https://github.com/loadder/Dynamic-Meta-filter" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Delving into Data: Effectively Substitute Training for Black-box Attack,</span>
<br> Wenxuan Wang, Bangjie Yin, Taiping Yao, Li Zhang, Yanwei Fu, Shouhong Ding, Jilin Li, Feiyue Huang, Xiangyang Xue
<br> CVPR 2021
</li> 


<li> <span class="papertitle">Learning a Few-shot Embedding Model with Contrastive Learning,</span>
<br> Chen Liu, Yanwei Fu, Chengming Xu, Siqian Yang, Jilin Li, Chengjie Wang, Li Zhang
<br> AAAI 2021
[<a href="https://github.com/corwinliu9669/Learning-a-Few-shot-Embedding-Model-with-Contrastive-Learning" target="_blank">code</a>]
</li> 



<li> <span class="papertitle">Long-Term Cloth-Changing Person Re-identification,</span>
<br> Xuelin Qian, Wenxuan Wang, Li Zhang, Fangrui Zhu, Yanwei Fu, Tao Xiang, Yu-Gang Jiang, Xiangyang Xue
<br> ACCV 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2005.12633" target="_blank">paper</a>]
[<a href="https://naiq.github.io/LTCC_Perosn_ReID.html" target="_blank">project page</a>]
</li> 


<li> <span class="papertitle">Dynamic Depth Fusion and Transformation for Monocular 3D Object Detection,</span>
<br> Erli Ouyang*, Li Zhang*, Mohan Chen, Anurag Arnab, Yanwei Fu
<br> ACCV 2020
[<a href="https://openaccess.thecvf.com/content/ACCV2020/papers/Ouyang_Dynamic_Depth_Fusion_and_Transformation_for_Monocular_3D_Object_Detection_ACCV_2020_paper.pdf" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Depth Guided Adaptive Meta-Fusion Network for Few-shot Video Recognition,</span>
<br> Yuqian Fu*, Li Zhang*, Junke Wang, Yanwei Fu, Yu-Gang Jiang
<br> ACM MM 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/2010.09982" target="_blank">paper</a>]
</li> 

<li> <span class="papertitle">Few-shot Action Recognition with Permutation-invariant Attention,</span>
<br> Hongguang Zhang, Li Zhang, Xiaojuan Qi, Hongdong Li, Philip H.S. Torr, Piotr Koniusz
<br> ECCV 2020 <span class="distinction">(Spotlight)</span> 
[<a href="https://arxiv.org/abs/2001.03905" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">XingGAN for Person Image Generation,</span>
<br> Hao Tang, Song Bai, Li Zhang, Philip H.S. Torr, Nicu Sebe
<br> ECCV 2020
[<a href="https://arxiv.org/abs/2007.09278" target="_blank">paper</a>]
[<a href="https://github.com/Ha0Tang/XingGAN" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Improving Semantic Segmentation via Decoupled Body and Edge Supervision,</span>
<br> Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping Shi, Zhouchen Lin, Shaohua Tan, Yunhai Tong
<br> ECCV 2020
[<a href="https://arxiv.org/abs/2007.10035" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/DecoupleSegNets" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Dynamic Graph Message Passing Network,</span>
<br> Li Zhang, Dan Xu, Anurag Arnab, Philip H.S. Torr
<br> CVPR 2020 <span class="distinction">(Oral)</span> 
[<a href="https://arxiv.org/abs/1908.06955" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/dgmn" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Strip Pooling: Rethinking Spatial Pooling for Scene Parsing,</span>
<br> Qibin Hou, Li Zhang, Ming-Ming Cheng, Jiashi Feng
<br> CVPR 2020
[<a href="https://arxiv.org/abs/2003.13328" target="_blank">paper</a>]
[<a href="https://github.com/Andrew-Qibin/SPNet" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Instance Credibility Inference for Few-Shot Learning,</span>
<br> Yikai Wang, Chengming Xu, Chen Liu, Li Zhang, Yanwei Fu
<br> CVPR 2020 
[<a href="https://arxiv.org/abs/2003.11853" target="_blank">paper</a>]
[<a href="https://github.com/Yikai-Wang/ICI-FSL" target="_blank">code</a>]
</li> 

<li> <span class="papertitle">Style Normalization and Restitution for Generalizable Person Re-identification,</span>
<br> Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, Li Zhang
<br> CVPR 2020 
[<a href="https://arxiv.org/abs/2005.11037" target="_blank">paper</a>]
</li> 


<li> <span class="papertitle">Dual Graph Convolutional Network for Semantic Segmentation,</span>
<br> Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yunhai Tong, Philip H.S. Torr
<br> BMVC 2019 
[<a href="https://arxiv.org/abs/1909.06121" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/GALD-Net" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Global Aggregation then Local Distribution in Fully Convolutional Networks,</span>
<br> Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan Yang, Yunhai Tong
<br> BMVC 2019 
[<a href="https://arxiv.org/abs/1909.07229" target="_blank">paper</a>]
[<a href="https://github.com/lxtGH/GALD-Net" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Fast Online Object Tracking and Segmentation: A Unifying Approach,</span>
<br> Qiang Wang*, Li Zhang*, Luca Bertinetto*, Weiming Hu, Philip H.S. Torr
<br> CVPR 2019 
[<a href="https://arxiv.org/abs/1812.05050" target="_blank">paper</a>]
[<a href="https://github.com/foolwood/SiamMask" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning to Compare: Relation Network for Few-Shot Learning,</span>
<br> Flood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip H.S. Torr, Timothy M. Hospedales
<br> CVPR 2018
[<a href="https://arxiv.org/abs/1711.06025" target="_blank">paper</a>]
[<a href="https://github.com/songrotek/LearningToCompare_FSL" target="_blank">FSL code</a>]
[<a href="https://github.com/lzrobots/LearningToCompare_ZSL" target="_blank">ZSL code</a>]
</li> 


<li> <span class="papertitle">Learning a Deep Embedding Model for Zero-Shot Learning,</span>
<br> Li Zhang, Tao Xiang, Shaogang Gong
<br> CVPR 2017
[<a href="https://arxiv.org/abs/1611.05088" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/DeepEmbeddingModel_ZSL" target="_blank">code</a>]
</li> 


<li> <span class="papertitle">Learning a Discriminative Null Space for Person Re-identification,</span>
<br> Li Zhang, Tao Xiang, Shaogang Gong
<br> CVPR 2016
[<a href="http://arxiv.org/abs/1603.02139" target="_blank">paper</a>]
[<a href="https://github.com/lzrobots/NullSpace_ReID" target="_blank">code</a>]
</li> 

<br> 


</ul> </div> 


<hr><div id="footer" class="row-footer"> 
<p>
&copy; <a href="https://lzrobots.github.io">Li Zhang</a> 2018-2023.
Based on a design by <a href="http://www.robots.ox.ac.uk/~namhoon/">Namhoon Lee</a>.
</p>
</div> 


</div>
</body> 
</html>






